{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFUNjZj1zjSLp9top45ahn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vagzikopis/SparkWordCount/blob/main/WordCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"SherlockAssignment\").getOrCreate()"
      ],
      "metadata": {
        "id": "JxO9C9j-elRz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load data from the file (Colab path)\n",
        "text_rdd = sc.textFile(\"/content/SherlockHolmes.txt\")\n",
        "\n",
        "def process_words(line):\n",
        "    # Clean and lowercase\n",
        "    clean_line = re.sub(r'[^\\w\\s]', '', line.lower())\n",
        "    return clean_line.split()\n",
        "\n",
        "# Execute the following transformation logic\n",
        "# 1. Break lines into words and create a flat list. Apply processing function on words.\n",
        "# 2. Filter out empty strings and keep only words that start with a-z characters.\n",
        "# 3. Create a key-value pair with the first character of the word and (word length, 1).\n",
        "# 4. Reduce-by-key groups all tuples by their key (first letter). Afterwards, sums the total length and the word counts.\n",
        "# 5. Divide total word lengths with the total word counts per character.\n",
        "# 6. Reorder the RDD based on the average length per character in descending order.\n",
        "# 7. Action to pull the data from the cluster and return it to a local Python list.\n",
        "results = text_rdd.flatMap(process_words) \\\n",
        ".filter(lambda word: word and word[0].isalpha()) \\\n",
        ".map(lambda word: (word[0], (len(word), 1))) \\\n",
        ".reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
        ".mapValues(lambda x: x[0] / x[1]) \\\n",
        ".sortBy(lambda x: x[1], ascending=False) \\\n",
        ".collect()\n",
        "\n",
        "# Print the results\n",
        "for char, avg in results:\n",
        "    print(f\"{char} {avg:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80qffBFOfEmn",
        "outputId": "e67e9d2d-0055-4160-a1b1-7907b0650df3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c 7.2\n",
            "e 7.1\n",
            "q 7.0\n",
            "p 7.0\n",
            "r 6.9\n",
            "d 6.4\n",
            "v 6.0\n",
            "g 5.9\n",
            "s 5.8\n",
            "z 5.7\n",
            "u 5.6\n",
            "j 5.6\n",
            "k 5.4\n",
            "l 5.3\n",
            "f 5.2\n",
            "m 5.1\n",
            "n 4.8\n",
            "b 4.5\n",
            "w 4.3\n",
            "h 3.8\n",
            "a 3.7\n",
            "y 3.7\n",
            "t 3.6\n",
            "i 3.5\n",
            "x 3.4\n",
            "o 3.0\n"
          ]
        }
      ]
    }
  ]
}